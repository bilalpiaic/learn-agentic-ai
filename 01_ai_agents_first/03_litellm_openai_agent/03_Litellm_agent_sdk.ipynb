{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PdKwzEluDBN7"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bilalpiaic/learn-agentic-ai/blob/main/01_ai_agents_first/03_litellm_openai_agent/03_Litellm_agent_sdk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install openai-agents SDK"
      ],
      "metadata": {
        "id": "PdKwzEluDBN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Install openai-agents SDK\n",
        "\n",
        "!pip install -Uq openai-agents\n"
      ],
      "metadata": {
        "id": "fBC1_wgz7o62"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq openai-agents  \"openai-agents[litellm]\""
      ],
      "metadata": {
        "id": "3QdkOviEB2ay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71804502-8ef4-42a2-ec06-cc1f6bd0fa9b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make your Jupyter Notebook capable of running asynchronous functions."
      ],
      "metadata": {
        "id": "7yD91lz4DIAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Make your Jupyter Notebook capable of running asynchronous functions.\n",
        "\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n"
      ],
      "metadata": {
        "id": "G6dKW6Fk75al"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "7A5YLi3HCfBV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Google Gemini with LiteLLm and OPENAI-Agent SDK"
      ],
      "metadata": {
        "id": "K3VTUWDaGFcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Run Sync"
      ],
      "metadata": {
        "id": "P_zEYQa1kHC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prompt: explain  from __future__ import annotations\n",
        "\n",
        "The line `from __future__ import annotations` is used for type hinting in Python.  Let's break down why it's there and what it does.\n",
        "\n",
        "**Type Hinting in Python**\n",
        "\n",
        "Type hinting is a way to specify the expected data types of variables, function arguments, and return values in your Python code. This improves code readability, helps catch errors early during development, and assists static analysis tools in identifying potential issues.\n",
        "\n",
        "**The Problem with Forward References**\n",
        "\n",
        "In Python, you sometimes need to refer to a class or type that hasn't been fully defined yet.  This is common when classes are interconnected (e.g., a class `A` has a method that returns an instance of class `B`, but `B` is defined *after* `A`). This is known as a forward reference.\n",
        "\n",
        "Before Python 3.7, the type hint would have been evaluated at the time it's encountered in the code, rather than at runtime. This meant that forward references wouldn't work correctly, leading to errors: if Class B was defined after Class A in the code, then the type hint referring to B in A would not be recognized.\n",
        "\n",
        "**The Solution: `from __future__ import annotations`**\n",
        "\n",
        "This line tells the Python interpreter to use the new behavior for type hinting introduced in Python 3.7.  Specifically, it postpones the evaluation of type hints to runtime.  This makes it possible to correctly use forward references without errors.\n",
        "\n",
        "**Why It's (Likely) There**\n",
        "\n",
        "In the provided code snippet, although there are no explicit type hints visible, this import statement is good practice.  It ensures that if type hints are added later, they will work correctly regardless of the order in which classes are defined within the file. Including this line future-proofs the code for any type hints that may be added later.  It makes no difference in this example, but it would be important if you later added types.\n"
      ],
      "metadata": {
        "id": "o-CjtWFSAYIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from agents import Agent, Runner, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "@function_tool\n",
        "def get_weather(city: str)->str:\n",
        "    print(f\"[debug] getting weather for {city}\")\n",
        "    return f\"The weather in {city} is sunny.\"\n",
        "\n",
        "def main(model: str, api_key: str):\n",
        "  agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You only respond in haikus.\",\n",
        "      model=LitellmModel(model=model, api_key=api_key),\n",
        "\n",
        "  )\n",
        "\n",
        "  result = Runner.run_sync(agent, \"What's the weather in Tokyo?\")\n",
        "  print(result.final_output)\n",
        "\n",
        "main(model=MODEL, api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuSbD2P780Ze",
        "outputId": "211e5635-bd4e-428c-dfee-94f13276230d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skies now start to clear,\n",
            "Sunlight warms the city's face,\n",
            "Gentle breeze arrives.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import asyncio\n",
        "\n",
        "from agents import Agent, Runner, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def get_weather(city: str)->str:\n",
        "    print(f\"[debug] getting weather for {city}\")\n",
        "    return f\"The weather in {city} is sunny.\"\n",
        "\n",
        "\n",
        "def main(model: str, api_key: str):\n",
        "  agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You only respond in haikus.\",\n",
        "      model=LitellmModel(model=model, api_key=api_key),\n",
        "\n",
        "  )\n",
        "\n",
        "  result = Runner.run_sync(agent, \"What's the weather in Tokyo?\")\n",
        "  print(result.final_output)\n",
        "\n",
        "\n",
        "main(model=MODEL, api_key=GEMINI_API_KEY)\n",
        "\n"
      ],
      "metadata": {
        "id": "WBT9Z8hE6kEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2163dcd5-41ca-47dc-86a1-7b36c88b02db"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skies are clear and bright,\n",
            "Gentle breeze, a sunny day,\n",
            "Warmth embraces all.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Async Function"
      ],
      "metadata": {
        "id": "S7ECiU-f5BAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import asyncio\n",
        "\n",
        "from agents import Agent, Runner, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GENIMI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def get_weather(city: str)->str:\n",
        "    print(f\"[debug] getting weather for {city}\")\n",
        "    return f\"The weather in {city} is sunny.\"\n",
        "\n",
        "async def main(model: str, api_key: str):\n",
        "  agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You only respond in haikus.\",\n",
        "      model=LitellmModel(model=model, api_key=api_key),\n",
        "  )\n",
        "\n",
        "  result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n",
        "  print(result.final_output)\n",
        "\n",
        "asyncio.run(main(model=MODEL, api_key=GEMINI_API_KEY))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN1j3SxI-5Nm",
        "outputId": "bdf00cff-96dd-4630-c572-9fa4b2434b5f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skies now softly weep,\n",
            "Cool air drifts through busy streets,\n",
            "Bring an umbrella.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import asyncio\n",
        "\n",
        "from agents import Agent, Runner, function_tool, set_tracing_disabled\n",
        "from agents.extensions.models.litellm_model import LitellmModel\n",
        "from google.colab import userdata\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "MODEL = 'gemini/gemini-2.0-flash'\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "\n",
        "@function_tool\n",
        "def get_weather(city: str)->str:\n",
        "    print(f\"[debug] getting weather for {city}\")\n",
        "    return f\"The weather in {city} is sunny.\"\n",
        "\n",
        "\n",
        "async def main(model: str, api_key: str):\n",
        "  agent = Agent(\n",
        "      name=\"Assistant\",\n",
        "      instructions=\"You only respond in haikus.\",\n",
        "      model=LitellmModel(model=model, api_key=api_key),\n",
        "\n",
        "  )\n",
        "\n",
        "  result = await Runner.run(agent, \"What's the weather in Tokyo?\")\n",
        "  print(result.final_output)\n",
        "\n",
        "\n",
        "asyncio.run(main(model=MODEL, api_key=GEMINI_API_KEY))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-QbKRj7wSzr",
        "outputId": "147c8603-a5d3-417a-b931-85a06fbe66df"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skies clear, gentle breeze,\n",
            "Sun warms the city brightly,\n",
            "A pleasant new day.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VfJfUqsH5Mfy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}